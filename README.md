# deepest-learning
Deep Learning Project Spring 2022

## Group Members
Ong Kah Yuan, Joel (1004366)
Mah Qing Long Hannah Jean (1004332)
Gwee Yong Ta (1004114)
Tan Jianhui (1004380)
Jerome Heng Hao Xiang (1004115)

## Overview
This approach realises voice to choral conversion by converting a single voice sample to several voice samples and then overlaying them to create a choral output.

## Requirements
Please install the required libraries in `requirements.txt`. Do note that the version of pytorch is important if you wish to reload the model using our checkpoints. 

## Preprocessing
The preprocessing script takes in the NUS dataset (which should be in `datasets\nus-smc-corpus_48`) with .wav files and phoneme annotations, extracts features and saves them as a .hdf5 representation. The .hdf files are saved to `ss_synthesis/voice`. 

As preprocessing takes some time, the processed hdf5 files can be downloaded from: . These should be saved under `ss_synthesis/voice` for training to proceed.
```
python preprocessing.py
```

## Training the model
To train the model from scratch, run the following command:
```
python main.py
```

If you would like to reload the model from a particular checkpoint, ensure that the checkpoints are saved in `\model_save_dir\` and run the following command:
```
python main.py --reload_model={N}
```
where N is an integer corresponding to the checkpoint. For instance, to reload the model from the checkpoint corresponding to epoch 549, run the following command:
```
python main.py --reload_model=549
```
The training will resume starting from epoch 550.

As training will also take some time, the required checkpoint files can be downloaded from: https://drive.google.com/file/d/1Y7v8Ce4yVG4Gmtbr_0j1fCNK9NbiYxVU/. These should be saved under `\model_save_dir\` for reloading to proceed.

## Generating the sound file
To generate the sound file after training the model, run the following command:
```
python main.py --reload_model={N} --eval y --source={source_file} --target={target_singer}
```
where N is the integer corresponding to the checkpoint, source_file is the source audio track file and target_singer is the target singer for voice conversion.
For instance, to convert a source file of AZIZ singing Edelweiss to SAMF:

```
python main.py --reload_model=949 --eval y --source=nus_ADIZ_sing_01.hdf5 --target=SAMF
```

## Generating choral output (audio merger)
A choral output can be generated by overlaying the generated sound files from a single source to multiple singers. This can be done by running the following command:
```
TODO: whats the command
```

## Plotting Loss and Wasserstein Distance
During training, various training losses, the wasserstein distance and the validation loss are saved into csv files in `/log_dir/`. To plot these graphs, run the following command:

```
python plot_histories_graph.py
```
The output graphs are saved in `/graph_dir/`. 

The logs and graphs from our training can be found in `/log_dir_saved/` and `/graph_dir_saved/` respectively. You may plot our graphs from our trained log file by running the following command:
```
python plot_histories_graph.py --log_dir=log_dir_saved
```

## Acknowledgements
The WGAN approach is based off the project: WGANSing: A Multi-Voice Singing Voice Synthesizer Based on the Wasserstein-GAN, found here: https://github.com/MTG/WGANSing. 

Much of the pre-processing and post-processing code is credited to them with some adjustments by the team, while the model itself (in /model) was completely reimplemented in pytorch by us.

## References
P. Chandna, M. Blaauw, J. Bonada and E. GÃ³mez, "WGANSing: A Multi-Voice Singing Voice Synthesizer Based on the Wasserstein-GAN," 2019 27th European Signal Processing Conference (EUSIPCO), 2019, pp. 1-5, doi: 10.23919/EUSIPCO.2019.8903099. 